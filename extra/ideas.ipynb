{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, p_lag:int, eps=1e-5, affine=True):\n",
    "        super(RevIN, self).__init__()\n",
    "        self.p_lag = p_lag\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode:str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features, self.p_lag))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features, self.p_lag))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        self.mean = torch.mean(x, dim=2, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=2, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x\n",
    "    \n",
    "class RLinear(nn.Module):\n",
    "    def __init__(self, p_lag, n_features, future_steps, batch_size = 8):\n",
    "        super(RLinear, self).__init__()\n",
    "        self.linear_layer = nn.Linear(p_lag * n_features, p_lag * 1)\n",
    "        self.revin_layer = RevIN(num_features = n_features, p_lag = p_lag)\n",
    "        self.criterion = nn.MSELoss()   \n",
    "        self.p_lag = p_lag\n",
    "        self.batch_size = batch_size\n",
    "        self.n_features = n_features\n",
    "        self.future_steps = future_steps\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.float()\n",
    "        input_normalized = self.revin_layer(input.reshape(self.batch_size,self.n_features,self.p_lag), 'norm')\n",
    "        input_transformed = self.linear_layer(input_normalized.reshape(self.batch_size, self.p_lag*self.n_features))\n",
    "        input_denormalized = self.revin_layer(input_transformed.reshape(self.batch_size,1,self.p_lag), 'denorm')  \n",
    "        return input_denormalized\n",
    "\n",
    "\n",
    "training_df, val_df, test_df = split_dataset(ETTm2)\n",
    "train_data = DataLoader(TimeSeriesDataset(training_df, future_steps= 7, target_column = ['OT'], p_lag=10), batch_size=2, drop_last=True)\n",
    "ri = RevIN(7,10, affine=True)\n",
    "rlinear = RLinear(10,7,7,2)\n",
    "for idx, (i,t) in enumerate(train_data): \n",
    "    #peter = ri(i.reshape(2,7,10), 'norm')\n",
    "    #print(peter)\n",
    "    #repeter = ri(peter.reshape(2,7,10), 'denorm')\n",
    "    #print(repeter)\n",
    "    print(rlinear(i))\n",
    "    if idx == 1: \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
