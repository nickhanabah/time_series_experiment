{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import split_dataset, TimeSeriesDataset\n",
    "from utils.evaluation_utils import plot_multistep_forecast\n",
    "#from utils.training_utils import train\n",
    "import pandas as pd \n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def normal_loss(normal_dist, y):\n",
    "    neg_log_likelihood = -normal_dist.log_prob(y)\n",
    "    return torch.mean(neg_log_likelihood)\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "\n",
    "class DecompositionLayer(nn.Module):\n",
    "    def __init__(self, kernel_size, n_features):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=1, padding=0)\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_of_pads = (self.kernel_size - 1) // 2\n",
    "        if self.kernel_size > self.n_features:\n",
    "            front = x[:, 0:1, :].repeat(1, num_of_pads + 1, 1)\n",
    "        else:\n",
    "            front = x[:, 0:1, :].repeat(1, num_of_pads, 1)\n",
    "        end = x[:, -1:, :].repeat(1, num_of_pads, 1)\n",
    "        x_padded = torch.cat([front, x, end], dim=1)\n",
    "        x_trend = self.avg(x_padded.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x_seasonal = x - x_trend\n",
    "        return x_seasonal, x_trend\n",
    "\n",
    "\n",
    "class ARNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        p_lag: int,\n",
    "        n_continous_features: int,\n",
    "        n_categorial_features: int,\n",
    "        future_steps: int,\n",
    "        decomp_kernel_size: int = 7,\n",
    "        batch_size: int = 8,\n",
    "        model: str = \"rlinear\",\n",
    "        optimization: str = \"mse\",\n",
    "        modelling_task: str = \"univariate\",\n",
    "        density: bool = False,\n",
    "        depth:str = 'shallow'\n",
    "    ):\n",
    "\n",
    "        super(ARNet, self).__init__()\n",
    "        self.model = model\n",
    "        self.optimization = optimization\n",
    "        self.n_categorial_features = n_categorial_features\n",
    "        self.modelling_task = modelling_task\n",
    "        self.density = density\n",
    "        self.depth = depth\n",
    "\n",
    "        if self.modelling_task == \"univariate\":\n",
    "            print(\"Univatiate modelling\")\n",
    "            self.inflation_factor = 1\n",
    "            print(f\"inflation factor = {self.inflation_factor}\")\n",
    "\n",
    "        elif self.modelling_task == \"multivariate\":\n",
    "            print(\"Multivariate modelling\")\n",
    "            self.inflation_factor = n_continous_features\n",
    "            print(f\"inflation factor = {self.inflation_factor}\")\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if self.optimization == \"mse\":\n",
    "            self.criterion = nn.MSELoss()\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if model == \"dlinear\":\n",
    "            print(\"Dlinear activated\")\n",
    "            self.decomp_layer = DecompositionLayer(\n",
    "                decomp_kernel_size, n_continous_features\n",
    "            )\n",
    "            if self.density:\n",
    "                print(\"Density to be estimated\")\n",
    "                self.mu_trend_layer = nn.Linear(\n",
    "                    p_lag * (n_continous_features + n_categorial_features), 1\n",
    "                )\n",
    "                self.mu_seasonal_layer = nn.Linear(\n",
    "                    p_lag * (n_continous_features + n_categorial_features), 1\n",
    "                )\n",
    "                self.std_trend_layer = nn.Linear(\n",
    "                    p_lag * (n_continous_features + n_categorial_features), 1\n",
    "                )\n",
    "                self.std_seasonal_layer = nn.Linear(\n",
    "                    p_lag * (n_continous_features + n_categorial_features), 1\n",
    "                )\n",
    "            else:\n",
    "                print(\"Points to be estimated\")\n",
    "                if depth == 'deep': \n",
    "                    print(\"With a deep network\")\n",
    "                    self.relu = nn.ReLU()\n",
    "                    self.input_trend_1layer = nn.Linear(\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                    )\n",
    "                    self.input_seasonal_1layer = nn.Linear(\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                    )\n",
    "                    self.input_trend_2layer = nn.Linear(\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                    )\n",
    "                    self.input_seasonal_2layer = nn.Linear(\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                    )\n",
    "                    self.input_trend_3layer = nn.Linear(\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                    )\n",
    "                    self.input_seasonal_3layer = nn.Linear(\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                    )\n",
    "                    self.input_trend_4layer = nn.Linear(\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                        future_steps * self.inflation_factor,\n",
    "                    )\n",
    "                    self.input_seasonal_4layer = nn.Linear(\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                        future_steps * self.inflation_factor,\n",
    "                    )\n",
    "                elif depth == 'shallow': \n",
    "                    print(\"With a shallow network\")\n",
    "                    self.input_trend_layer = nn.Linear(\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                        future_steps * self.inflation_factor,\n",
    "                    )\n",
    "                    self.input_seasonal_layer = nn.Linear(\n",
    "                        p_lag * (n_continous_features + n_categorial_features),\n",
    "                        future_steps * self.inflation_factor,\n",
    "                    )\n",
    "                else: \n",
    "                    raise NotImplementedError\n",
    "\n",
    "        elif model == \"rlinear\":\n",
    "            print(\"Rlinear activated\")\n",
    "            # if self.density:\n",
    "            #    print('Density to be estimated')\n",
    "            #    self.mu_layer = nn.Linear(p_lag * (n_continous_features + n_categorial_features), 1)\n",
    "            #    self.std_layer = nn.Linear(p_lag * (n_continous_features + n_categorial_features), 1)\n",
    "            # else:\n",
    "            print(\"Points to be estimated\")\n",
    "            self.input_layer = nn.Linear(\n",
    "                p_lag * (n_continous_features + n_categorial_features),\n",
    "                future_steps * self.inflation_factor,\n",
    "            )\n",
    "\n",
    "        elif model == \"rmlp\":\n",
    "            print(\"RMLP activated\")\n",
    "            self.input_layer = nn.Linear(\n",
    "                p_lag * (n_continous_features + n_categorial_features),\n",
    "                p_lag * (n_continous_features + n_categorial_features),\n",
    "            )\n",
    "            self.relu = nn.ReLU()\n",
    "            # if self.density:\n",
    "            #    print('Density to be estimated')\n",
    "            #    self.mu_layer = nn.Linear(p_lag * (n_continous_features + n_categorial_features), 1)\n",
    "            #    self.std_layer = nn.Linear(p_lag * (n_continous_features + n_categorial_features), 1)\n",
    "            # else:\n",
    "            print(\"Points to be estimated\")\n",
    "            self.output_layer = nn.Linear(\n",
    "                p_lag * (n_continous_features + n_categorial_features),\n",
    "                future_steps * self.inflation_factor,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.p_lag = p_lag\n",
    "        self.batch_size = batch_size\n",
    "        self.n_continous_features = n_continous_features\n",
    "        self.future_steps = future_steps\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.float()\n",
    "        new_input = input.reshape(\n",
    "            self.batch_size,\n",
    "            (self.n_continous_features + self.n_categorial_features),\n",
    "            self.p_lag,\n",
    "        )\n",
    "\n",
    "        if self.model == \"rlinear\":\n",
    "            continous_input = new_input[:, 0 : (self.n_continous_features), :]\n",
    "            categorial_input = new_input[\n",
    "                :,\n",
    "                self.n_continous_features : (\n",
    "                    self.n_continous_features + self.n_categorial_features\n",
    "                ),\n",
    "                :,\n",
    "            ]\n",
    "\n",
    "            # continous_input tranformation\n",
    "            mean_values = torch.mean(continous_input, dim=2).reshape(\n",
    "                self.batch_size, self.n_continous_features, 1\n",
    "            )\n",
    "            mean_adj_input = continous_input - mean_values\n",
    "            std_values = torch.std(continous_input, dim=2).reshape(\n",
    "                self.batch_size, self.n_continous_features, 1\n",
    "            )\n",
    "            eps_values = torch.full((self.batch_size, self.n_continous_features, 1), 1)\n",
    "            standardized_input = mean_adj_input / (std_values + eps_values)\n",
    "\n",
    "            # put all parts together again\n",
    "            standardized_input = torch.cat((standardized_input, categorial_input), 1)\n",
    "            standardized_input = self.dropout(standardized_input)\n",
    "\n",
    "            y_hat = self.input_layer(\n",
    "                standardized_input.reshape(\n",
    "                    self.batch_size,\n",
    "                    self.p_lag\n",
    "                    * (self.n_continous_features + self.n_categorial_features),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if self.modelling_task == \"univariate\":\n",
    "                rev_mean = mean_values.squeeze(2)[\n",
    "                    :, self.n_continous_features - 1\n",
    "                ].reshape(self.batch_size, 1)\n",
    "                rev_std = std_values.squeeze(2)[\n",
    "                    :, self.n_continous_features - 1\n",
    "                ].reshape(self.batch_size, 1)\n",
    "                rev_eps = torch.full((self.batch_size, 1), 1)\n",
    "            elif self.modelling_task == \"multivariate\":\n",
    "                rev_mean_l = []\n",
    "                for tensor in mean_values.reshape(\n",
    "                    self.batch_size, self.n_continous_features\n",
    "                ):\n",
    "                    [\n",
    "                        rev_mean_l.append(\n",
    "                            torch.full((self.future_steps, 1), i.item()).reshape(\n",
    "                                self.future_steps\n",
    "                            )\n",
    "                        )\n",
    "                        for i in tensor\n",
    "                    ]\n",
    "                rev_mean = torch.cat(rev_mean_l).reshape(\n",
    "                    self.batch_size, self.n_continous_features * self.future_steps\n",
    "                )\n",
    "                rev_std_l = []\n",
    "                for tensor in std_values.reshape(\n",
    "                    self.batch_size, self.n_continous_features\n",
    "                ):\n",
    "                    [\n",
    "                        rev_std_l.append(\n",
    "                            torch.full((self.future_steps, 1), i.item()).reshape(\n",
    "                                self.future_steps\n",
    "                            )\n",
    "                        )\n",
    "                        for i in tensor\n",
    "                    ]\n",
    "                rev_std = torch.cat(rev_std_l).reshape(\n",
    "                    self.batch_size, self.n_continous_features * self.future_steps\n",
    "                )\n",
    "                rev_eps = torch.full(\n",
    "                    (self.batch_size, self.n_continous_features * self.future_steps), 1\n",
    "                )\n",
    "            else:\n",
    "                NotImplementedError\n",
    "\n",
    "            y_hat = y_hat * (rev_std + rev_eps) + rev_mean\n",
    "\n",
    "        elif self.model == \"dlinear\":\n",
    "            continous_input = new_input[:, 0 : (self.n_continous_features), :]\n",
    "            categorial_input = new_input[\n",
    "                :,\n",
    "                self.n_continous_features : (\n",
    "                    self.n_continous_features + self.n_categorial_features\n",
    "                ),\n",
    "                :,\n",
    "            ]\n",
    "\n",
    "            # continous_input tranformation\n",
    "            input_season, input_trend = self.decomp_layer(continous_input)\n",
    "            input_season = torch.cat((input_season, categorial_input), 1)\n",
    "            input_trend = torch.cat((input_trend, categorial_input), 1)\n",
    "            input_season = self.dropout(input_season)\n",
    "            input_trend = self.dropout(input_trend)\n",
    "\n",
    "            if self.density:\n",
    "                mu_trend = self.mu_trend_layer(\n",
    "                    input_trend.reshape(\n",
    "                        self.batch_size,\n",
    "                        self.p_lag\n",
    "                        * (self.n_continous_features + self.n_categorial_features),\n",
    "                    )\n",
    "                )\n",
    "                std_trend = self.std_trend_layer(\n",
    "                    input_trend.reshape(\n",
    "                        self.batch_size,\n",
    "                        self.p_lag\n",
    "                        * (self.n_continous_features + self.n_categorial_features),\n",
    "                    )\n",
    "                )\n",
    "                mu_season = self.mu_seasonal_layer(\n",
    "                    input_season.reshape(\n",
    "                        self.batch_size,\n",
    "                        self.p_lag\n",
    "                        * (self.n_continous_features + self.n_categorial_features),\n",
    "                    )\n",
    "                )\n",
    "                std_season = self.std_seasonal_layer(\n",
    "                    input_season.reshape(\n",
    "                        self.batch_size,\n",
    "                        self.p_lag\n",
    "                        * (self.n_continous_features + self.n_categorial_features),\n",
    "                    )\n",
    "                )\n",
    "                self.sofplus = torch.nn.Softplus()\n",
    "            else:\n",
    "                if self.depth == 'deep': \n",
    "                    input_season_reshaped = input_season.reshape(\n",
    "                            self.batch_size,\n",
    "                            self.p_lag\n",
    "                            * (self.n_continous_features + self.n_categorial_features),\n",
    "                        )\n",
    "                    \n",
    "                    input_trend_reshaped = input_trend.reshape(\n",
    "                            self.batch_size,\n",
    "                            self.p_lag\n",
    "                            * (self.n_continous_features + self.n_categorial_features),\n",
    "                        )\n",
    "                    y_hat_season = self.input_seasonal_1layer(\n",
    "                        input_season_reshaped\n",
    "                    )\n",
    "                    y_hat_season = self.relu(self.input_seasonal_2layer(y_hat_season)) + input_season_reshaped\n",
    "                    y_hat_season = self.relu(self.input_seasonal_3layer(y_hat_season)) + input_season_reshaped\n",
    "                    y_hat_season = self.relu(self.input_seasonal_4layer(y_hat_season))\n",
    "\n",
    "                    y_hat_trend = self.input_trend_1layer(\n",
    "                        input_trend_reshaped\n",
    "                    )\n",
    "                    y_hat_trend = self.relu(self.input_trend_2layer(y_hat_trend)) + input_trend_reshaped\n",
    "                    y_hat_trend = self.relu(self.input_trend_3layer(y_hat_trend)) + input_trend_reshaped\n",
    "                    y_hat_trend = self.relu(self.input_trend_4layer(y_hat_trend))\n",
    "                elif self.depth == 'shallow': \n",
    "                    y_hat_season = self.input_seasonal_layer(\n",
    "                        input_season.reshape(\n",
    "                            self.batch_size,\n",
    "                            self.p_lag\n",
    "                            * (self.n_continous_features + self.n_categorial_features),\n",
    "                        )\n",
    "                    )\n",
    "                    y_hat_trend = self.input_trend_layer(\n",
    "                        input_trend.reshape(\n",
    "                            self.batch_size,\n",
    "                            self.p_lag\n",
    "                            * (self.n_continous_features + self.n_categorial_features),\n",
    "                        )\n",
    "                    )\n",
    "                else: \n",
    "                    raise NotImplementedError\n",
    "                y_hat = y_hat_season + y_hat_trend\n",
    "\n",
    "        elif self.model == \"rmlp\":\n",
    "            continous_input = new_input[:, 0 : (self.n_continous_features), :]\n",
    "            categorial_input = new_input[\n",
    "                :,\n",
    "                self.n_continous_features : (\n",
    "                    self.n_continous_features + self.n_categorial_features\n",
    "                ),\n",
    "                :,\n",
    "            ]\n",
    "\n",
    "            # continous_input tranformation\n",
    "            mean_values = torch.mean(continous_input, dim=2).reshape(\n",
    "                self.batch_size, self.n_continous_features, 1\n",
    "            )\n",
    "            mean_adj_input = continous_input - mean_values\n",
    "            std_values = torch.std(continous_input, dim=2).reshape(\n",
    "                self.batch_size, self.n_continous_features, 1\n",
    "            )\n",
    "            eps_values = torch.full((self.batch_size, self.n_continous_features, 1), 1)\n",
    "            standardized_input = mean_adj_input / (std_values + eps_values)\n",
    "\n",
    "            # put all parts together again\n",
    "            standardized_input = torch.cat((standardized_input, categorial_input), 1)\n",
    "            standardized_input = self.dropout(standardized_input)\n",
    "            y_hat = self.input_layer(\n",
    "                standardized_input.reshape(\n",
    "                    self.batch_size,\n",
    "                    self.p_lag\n",
    "                    * (self.n_continous_features + self.n_categorial_features),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # prediction part\n",
    "            standardized_input = self.dropout(standardized_input)\n",
    "            y_hat = self.relu(\n",
    "                self.input_layer(\n",
    "                    standardized_input.reshape(\n",
    "                        self.batch_size,\n",
    "                        self.p_lag\n",
    "                        * (self.n_continous_features + self.n_categorial_features),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            y_hat = (\n",
    "                y_hat.reshape(\n",
    "                    self.batch_size,\n",
    "                    (self.n_continous_features + self.n_categorial_features),\n",
    "                    self.p_lag,\n",
    "                )\n",
    "                + standardized_input\n",
    "            )\n",
    "            y_hat = self.output_layer(\n",
    "                y_hat.reshape(\n",
    "                    self.batch_size,\n",
    "                    self.p_lag\n",
    "                    * (self.n_continous_features + self.n_categorial_features),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if self.modelling_task == \"univariate\":\n",
    "                rev_mean = mean_values.squeeze(2)[\n",
    "                    :, self.n_continous_features - 1\n",
    "                ].reshape(self.batch_size, 1)\n",
    "                rev_std = std_values.squeeze(2)[\n",
    "                    :, self.n_continous_features - 1\n",
    "                ].reshape(self.batch_size, 1)\n",
    "                rev_eps = torch.full((self.batch_size, 1), 1)\n",
    "            elif self.modelling_task == \"multivariate\":\n",
    "                rev_mean_l = []\n",
    "                for tensor in mean_values.reshape(\n",
    "                    self.batch_size, self.n_continous_features\n",
    "                ):\n",
    "                    [\n",
    "                        rev_mean_l.append(\n",
    "                            torch.full((self.future_steps, 1), i.item()).reshape(\n",
    "                                self.future_steps\n",
    "                            )\n",
    "                        )\n",
    "                        for i in tensor\n",
    "                    ]\n",
    "                rev_mean = torch.cat(rev_mean_l).reshape(\n",
    "                    self.batch_size, self.n_continous_features * self.future_steps\n",
    "                )\n",
    "                rev_std_l = []\n",
    "                for tensor in std_values.reshape(\n",
    "                    self.batch_size, self.n_continous_features\n",
    "                ):\n",
    "                    [\n",
    "                        rev_std_l.append(\n",
    "                            torch.full((self.future_steps, 1), i.item()).reshape(\n",
    "                                self.future_steps\n",
    "                            )\n",
    "                        )\n",
    "                        for i in tensor\n",
    "                    ]\n",
    "                rev_std = torch.cat(rev_std_l).reshape(\n",
    "                    self.batch_size, self.n_continous_features * self.future_steps\n",
    "                )\n",
    "                rev_eps = torch.full(\n",
    "                    (self.batch_size, self.n_continous_features * self.future_steps), 1\n",
    "                )\n",
    "            else:\n",
    "                NotImplementedError\n",
    "\n",
    "            y_hat = y_hat * (rev_std + rev_eps) + rev_mean\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if self.density and self.model == \"dlinear\":\n",
    "            normal_object = torch.distributions.normal.Normal(\n",
    "                (mu_trend + mu_season),\n",
    "                (self.sofplus(std_trend) + self.sofplus(std_season)),\n",
    "            )\n",
    "            return normal_object\n",
    "        else:\n",
    "            return y_hat\n",
    "\n",
    "\n",
    "class Gating(nn.Module):\n",
    "    def __init__(self, input_dim,\n",
    "                 num_experts, dropout_rate=0.1):\n",
    "        super(Gating, self).__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.layer2 = nn.Linear(128, 256)\n",
    "        self.leaky_relu1 = nn.LeakyReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.leaky_relu2 = nn.LeakyReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.layer4 = nn.Linear(128, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.leaky_relu2(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        return torch.softmax(self.layer4(x), dim=1)\n",
    "\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, trained_experts:list[ARNet]):\n",
    "        super(MoE, self).__init__()\n",
    "        self.experts = nn.ModuleList(trained_experts)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        for expert in self.experts:\n",
    "            for param in expert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        num_experts = len(trained_experts)\n",
    "        input_dim = self.experts[0].p_lag * (self.experts[0].n_continous_features + self.experts[0].n_categorial_features)\n",
    "        self.gating = Gating(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('weights')\n",
    "        weights = self.gating(x)\n",
    "        #print(weights)\n",
    "        #print('----------------------------')\n",
    "        #print('outputs')\n",
    "        outputs = torch.stack(\n",
    "            [expert(x) for expert in self.experts], dim=2)\n",
    "        print(outputs.shape)\n",
    "        weights = torch.reshape(weights, outputs.shape)\n",
    "        print(weights.shape)\n",
    "        #weights = weights.unsqueeze(1).expand_as(outputs)\n",
    "        return torch.sum(outputs * weights, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.model_utils import ARNet, set_seed, normal_loss, MoE\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.data_utils import TimeSeriesDataset\n",
    "from utils.metrics import metric\n",
    "import random\n",
    "\n",
    "def train_expert_or_moe(\n",
    "        net,\n",
    "        learning_rate: float, \n",
    "        train_data:DataLoader, \n",
    "        modelling_task: str, \n",
    "        n_continous_features:int, \n",
    "        batch_size:int, \n",
    "        val_loss_list:list, \n",
    "        val_data:DataLoader, \n",
    "        train_loss_list:list, \n",
    "        i:int, \n",
    "        density: bool = False, \n",
    "        num_of_experts: int = 2, \n",
    "        epochs:int = 2,\n",
    "        moe: bool = False): \n",
    "    \n",
    "    if moe == False: \n",
    "        print(f'Started training expert {i +1}/{num_of_experts}')\n",
    "    else: \n",
    "        print('Started training Mixture of Experts')\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        train_counter = 0\n",
    "        val_counter = 0\n",
    "        running_train_loss = 0.0\n",
    "        running_val_loss = 0.0\n",
    "        running_train_mae = 0.0\n",
    "        running_train_mse = 0.0\n",
    "        running_val_mae = 0.0\n",
    "        running_val_mse = 0.0\n",
    "\n",
    "        if epoch + 1 != 1 and (epoch + 1) % 2 == 0:\n",
    "            learning_rate = learning_rate / 2\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        print(f\"Current learning rate is : {learning_rate}\")\n",
    "        print(\"---------------------------\")\n",
    "        for i, data in enumerate(train_data):\n",
    "            inputs, labels = data\n",
    "            labels = labels.squeeze(0).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            if density:\n",
    "                loss = normal_loss(outputs, labels.squeeze(1))\n",
    "            else:\n",
    "                if modelling_task == \"multivariate\":\n",
    "                    loss = net.criterion(\n",
    "                        outputs,\n",
    "                        labels[:, 0:(n_continous_features), :].reshape(outputs.shape),\n",
    "                        )\n",
    "                else:\n",
    "                    loss = net.criterion(outputs, labels.squeeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            if density == False:\n",
    "                outputs_array = outputs.detach().cpu().numpy()\n",
    "                labels_array = labels.squeeze(2).detach().cpu().numpy()\n",
    "                mae, mse = metric(pred=outputs_array, true=labels_array)\n",
    "                running_train_mae += mae\n",
    "                running_train_mse += mse\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "            train_counter += batch_size\n",
    "            if train_counter % 5000 == 0:\n",
    "                print(\n",
    "                    f\"Current (running) training loss at iteration {train_counter} : {running_train_loss/train_counter}\"\n",
    "                    )\n",
    "        train_loss_list.append(running_train_loss / train_counter)\n",
    "\n",
    "        if density == False:\n",
    "            for i, data in enumerate(val_data):\n",
    "                inputs, test_labels = data\n",
    "                test_labels = test_labels.squeeze(0).float()\n",
    "                output = net(inputs)\n",
    "                if modelling_task == \"multivariate\":\n",
    "                    val_loss = net.criterion(\n",
    "                        outputs,\n",
    "                        test_labels[:, 0:(n_continous_features), :].reshape(\n",
    "                            outputs.shape\n",
    "                            ),\n",
    "                        )\n",
    "                else:\n",
    "                    val_loss = net.criterion(output, test_labels.squeeze(1))\n",
    "\n",
    "                running_val_loss += val_loss.item()\n",
    "\n",
    "                output_array = output.detach().cpu().numpy()\n",
    "                test_labels_array = test_labels.squeeze(2).detach().cpu().numpy()\n",
    "                mae, mse = metric(pred=output_array, true=test_labels_array)\n",
    "                running_val_mae += mae\n",
    "                running_val_mse += mse\n",
    "                val_counter += batch_size\n",
    "            val_loss_list.append(running_val_loss / val_counter)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"\")\n",
    "                print(f\"Epoch {epoch}: \")\n",
    "                print(\"\")\n",
    "                print(\"Train metrics: -------\")\n",
    "                print(f\"Running (training) loss is {running_train_loss/train_counter}.\")\n",
    "                print(f\"Training MAE is {running_train_mae/train_counter}.\")\n",
    "                print(f\"Training MSE is {running_train_mse/train_counter}.\")\n",
    "                print(\"\")\n",
    "                print(\"Test metrics: -------\")\n",
    "                print(f\"Running (test) loss is {running_val_loss/val_counter}.\")\n",
    "                print(f\"Test MAE is {running_val_mae/val_counter}.\")\n",
    "                print(f\"Test MSE is {running_val_mse/val_counter}.\")\n",
    "                print(\"---------------------------\")\n",
    "    return net\n",
    "\n",
    "def train(\n",
    "    epochs,\n",
    "    p_lag,\n",
    "    future_steps,\n",
    "    n_continous_features,\n",
    "    n_categorial_features,\n",
    "    training_df,\n",
    "    validation_df,\n",
    "    feature_columns,\n",
    "    dataset_name: str,\n",
    "    target_column=[\"OT\"],\n",
    "    learning_rate=1.0e-4,\n",
    "    decomp_kernel_size=7,\n",
    "    batch_size=8,\n",
    "    model=\"rlinear\",\n",
    "    moe: bool = False, \n",
    "    num_of_experts: int = 1, \n",
    "    modelling_task=\"univariate\",\n",
    "    density=False,\n",
    "    depth = 'shallow'\n",
    "):\n",
    "\n",
    "    untrained_experts = []\n",
    "    for _ in range(num_of_experts): \n",
    "        set_seed(random.randint(3, 1000))\n",
    "        net = ARNet(\n",
    "                p_lag=p_lag,\n",
    "                n_continous_features=n_continous_features,\n",
    "                n_categorial_features=n_categorial_features,\n",
    "                future_steps=future_steps,\n",
    "                decomp_kernel_size=decomp_kernel_size,\n",
    "                batch_size=batch_size,\n",
    "                model=model,\n",
    "                modelling_task=modelling_task,\n",
    "                density=density,\n",
    "                depth = depth\n",
    "            )\n",
    "        untrained_experts.append(net)\n",
    "\n",
    "    train_data = DataLoader(\n",
    "        TimeSeriesDataset(\n",
    "            training_df,\n",
    "            future_steps=future_steps,\n",
    "            feature_columns=feature_columns,\n",
    "            target_column=target_column,\n",
    "            p_lag=p_lag,\n",
    "            modelling_task=modelling_task,\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    train_loss_list = []\n",
    "    val_data = DataLoader(\n",
    "        TimeSeriesDataset(\n",
    "            validation_df,\n",
    "            future_steps=future_steps,\n",
    "            feature_columns=feature_columns,\n",
    "            target_column=target_column,\n",
    "            p_lag=p_lag,\n",
    "            modelling_task=modelling_task,\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_loss_list = []\n",
    "\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    trained_experts = []\n",
    "    for i in range(num_of_experts): \n",
    "        net = train_expert_or_moe(\n",
    "                 learning_rate=learning_rate, \n",
    "                 num_of_experts=num_of_experts, \n",
    "                 net=untrained_experts[i], \n",
    "                 epochs=epochs, \n",
    "                 train_data=train_data, \n",
    "                 density=density, \n",
    "                 modelling_task=modelling_task, \n",
    "                 n_continous_features=n_continous_features, \n",
    "                 batch_size=batch_size, \n",
    "                 val_loss_list=val_loss_list, \n",
    "                 val_data=val_data, \n",
    "                 train_loss_list=train_loss_list, \n",
    "                 i=i, \n",
    "                 moe = False)\n",
    "        trained_experts.append(net)\n",
    "    if moe: \n",
    "        moe_model = MoE(trained_experts)\n",
    "        moe_model = train_expert_or_moe(\n",
    "            num_of_experts=num_of_experts, \n",
    "            learning_rate=learning_rate, \n",
    "            net=moe_model, \n",
    "            epochs=epochs, \n",
    "            train_data=train_data, \n",
    "            density=density, \n",
    "            modelling_task=modelling_task, \n",
    "            n_continous_features=n_continous_features, \n",
    "            batch_size=batch_size, \n",
    "            val_loss_list=val_loss_list, \n",
    "            val_data=val_data, \n",
    "            train_loss_list=train_loss_list, \n",
    "            i=0, \n",
    "            moe = True)\n",
    "        return moe_model\n",
    "    \n",
    "    else: \n",
    "        return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETTm2 = pd.read_csv(\"/workspaces/time_series_experiment/ETT-small/ETTm2.csv\")\n",
    "training_df, test_df = split_dataset(ETTm2, remain_same = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lag = 48\n",
    "future_steps = 24\n",
    "batch_size = 8\n",
    "epochs = 1\n",
    "learning_rate=1.e-4\n",
    "decomp_kernel_size = 24\n",
    "number_of_forecasts = 100\n",
    "target_column = ['OT']\n",
    "feature_columns = [i for i in training_df.columns]\n",
    "modelling_task = 'univariate'\n",
    "n_continous_features=7\n",
    "n_categorial_features=5\n",
    "dataset_name = 'ETTm2LongTraining'\n",
    "moe = True\n",
    "number_of_experts = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 244\n",
      "Univatiate modelling\n",
      "inflation factor = 1\n",
      "Rlinear activated\n",
      "Points to be estimated\n",
      "Random seed set as 971\n",
      "Univatiate modelling\n",
      "inflation factor = 1\n",
      "Rlinear activated\n",
      "Points to be estimated\n",
      "Started training expert 1/2\n",
      "Current learning rate is : 0.0001\n",
      "---------------------------\n",
      "Current (running) training loss at iteration 5000 : 16.95312412471771\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/time_series_experiment/moe_training.ipynb Zelle 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m net \u001b[39m=\u001b[39m train(\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m             epochs \u001b[39m=\u001b[39;49m epochs, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m             n_continous_features\u001b[39m=\u001b[39;49mn_continous_features, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m             n_categorial_features\u001b[39m=\u001b[39;49mn_categorial_features,\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m             p_lag\u001b[39m=\u001b[39;49m  p_lag, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m             future_steps \u001b[39m=\u001b[39;49m future_steps, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m             training_df \u001b[39m=\u001b[39;49m training_df, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             validation_df \u001b[39m=\u001b[39;49m test_df, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m             feature_columns \u001b[39m=\u001b[39;49m feature_columns,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m             target_column \u001b[39m=\u001b[39;49m target_column, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m             learning_rate\u001b[39m=\u001b[39;49mlearning_rate ,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m             decomp_kernel_size\u001b[39m=\u001b[39;49m decomp_kernel_size, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m             model \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mrlinear\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m             modelling_task \u001b[39m=\u001b[39;49m modelling_task, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m             dataset_name \u001b[39m=\u001b[39;49m dataset_name, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m             moe \u001b[39m=\u001b[39;49m moe, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m             num_of_experts \u001b[39m=\u001b[39;49m number_of_experts\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m             )\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m test_data \u001b[39m=\u001b[39m DataLoader(TimeSeriesDataset(test_df, future_steps\u001b[39m=\u001b[39m future_steps, target_column \u001b[39m=\u001b[39m target_column,feature_columns\u001b[39m=\u001b[39mfeature_columns,p_lag\u001b[39m=\u001b[39mp_lag), batch_size\u001b[39m=\u001b[39mbatch_size,drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m plot_multistep_forecast(test_data\u001b[39m=\u001b[39mtest_data, dataset_name \u001b[39m=\u001b[39m dataset_name, neural_net\u001b[39m=\u001b[39mnet, future_steps\u001b[39m=\u001b[39mfuture_steps, number_of_forecasts\u001b[39m=\u001b[39mnumber_of_forecasts)\n",
      "\u001b[1;32m/workspaces/time_series_experiment/moe_training.ipynb Zelle 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=187'>188</a>\u001b[0m trained_experts \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=188'>189</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_of_experts): \n\u001b[0;32m--> <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=189'>190</a>\u001b[0m     net \u001b[39m=\u001b[39m train_expert_or_moe(\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=190'>191</a>\u001b[0m              learning_rate\u001b[39m=\u001b[39;49mlearning_rate, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=191'>192</a>\u001b[0m              num_of_experts\u001b[39m=\u001b[39;49mnum_of_experts, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=192'>193</a>\u001b[0m              net\u001b[39m=\u001b[39;49muntrained_experts[i], \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=193'>194</a>\u001b[0m              epochs\u001b[39m=\u001b[39;49mepochs, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=194'>195</a>\u001b[0m              train_data\u001b[39m=\u001b[39;49mtrain_data, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=195'>196</a>\u001b[0m              density\u001b[39m=\u001b[39;49mdensity, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=196'>197</a>\u001b[0m              modelling_task\u001b[39m=\u001b[39;49mmodelling_task, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=197'>198</a>\u001b[0m              n_continous_features\u001b[39m=\u001b[39;49mn_continous_features, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=198'>199</a>\u001b[0m              batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=199'>200</a>\u001b[0m              val_loss_list\u001b[39m=\u001b[39;49mval_loss_list, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=200'>201</a>\u001b[0m              val_data\u001b[39m=\u001b[39;49mval_data, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=201'>202</a>\u001b[0m              train_loss_list\u001b[39m=\u001b[39;49mtrain_loss_list, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=202'>203</a>\u001b[0m              i\u001b[39m=\u001b[39;49mi, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=203'>204</a>\u001b[0m              moe \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=204'>205</a>\u001b[0m     trained_experts\u001b[39m.\u001b[39mappend(net)\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=205'>206</a>\u001b[0m \u001b[39mif\u001b[39;00m moe: \n",
      "\u001b[1;32m/workspaces/time_series_experiment/moe_training.ipynb Zelle 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mif\u001b[39;00m density \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-fishstick-57pv67j75vwfp6rj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     outputs_array \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    485\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m\"\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         state_steps,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     adam(\n\u001b[1;32m    227\u001b[0m         params_with_grad,\n\u001b[1;32m    228\u001b[0m         grads,\n\u001b[1;32m    229\u001b[0m         exp_avgs,\n\u001b[1;32m    230\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    231\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    232\u001b[0m         state_steps,\n\u001b[1;32m    233\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    234\u001b[0m         has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[1;32m    235\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    236\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    237\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    238\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    239\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    240\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    242\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    243\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    246\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m disabled_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/optim/adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 766\u001b[0m func(\n\u001b[1;32m    767\u001b[0m     params,\n\u001b[1;32m    768\u001b[0m     grads,\n\u001b[1;32m    769\u001b[0m     exp_avgs,\n\u001b[1;32m    770\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    771\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    772\u001b[0m     state_steps,\n\u001b[1;32m    773\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    774\u001b[0m     has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[1;32m    775\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    776\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    777\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    778\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    779\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    780\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    781\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    782\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    783\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    784\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    785\u001b[0m )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/optim/adam.py:433\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 433\u001b[0m     param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n\u001b[1;32m    435\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39mif\u001b[39;00m amsgrad \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = train(\n",
    "            epochs = epochs, \n",
    "            n_continous_features=n_continous_features, \n",
    "            n_categorial_features=n_categorial_features,\n",
    "            p_lag=  p_lag, \n",
    "            future_steps = future_steps, \n",
    "            training_df = training_df, \n",
    "            validation_df = test_df, \n",
    "            feature_columns = feature_columns,\n",
    "            target_column = target_column, \n",
    "            learning_rate=learning_rate ,\n",
    "            decomp_kernel_size= decomp_kernel_size, \n",
    "            batch_size=batch_size, \n",
    "            model = 'rlinear', \n",
    "            modelling_task = modelling_task, \n",
    "            dataset_name = dataset_name, \n",
    "            moe = moe, \n",
    "            num_of_experts = number_of_experts\n",
    "            )\n",
    "test_data = DataLoader(TimeSeriesDataset(test_df, future_steps= future_steps, target_column = target_column,feature_columns=feature_columns,p_lag=p_lag), batch_size=batch_size,drop_last=True)\n",
    "plot_multistep_forecast(test_data=test_data, dataset_name = dataset_name, neural_net=net, future_steps=future_steps, number_of_forecasts=number_of_forecasts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
