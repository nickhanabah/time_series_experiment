{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import split_dataset, TimeSeriesDataset\n",
    "from utils.evaluation_utils import plot_multistep_forecast\n",
    "from utils.training_utils import train\n",
    "import pandas as pd \n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETTm2 = pd.read_csv(\"/workspaces/time_series_experiment/ETT-small/ETTm2.csv\")\n",
    "training_df, test_df = split_dataset(ETTm2, remain_same = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lag = 48\n",
    "future_steps = 24\n",
    "batch_size = 8\n",
    "epochs = 1\n",
    "learning_rate=1.e-4\n",
    "decomp_kernel_size = 24\n",
    "number_of_forecasts = 100\n",
    "target_column = ['OT']\n",
    "feature_columns = [i for i in training_df.columns]\n",
    "modelling_task = 'univariate'\n",
    "n_continous_features=7\n",
    "n_categorial_features=5\n",
    "dataset_name = 'ETTm2LongTraining'\n",
    "moe = True\n",
    "number_of_experts = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 6\n",
      "Univatiate modelling\n",
      "inflation factor = 1\n",
      "Rlinear activated\n",
      "Points to be estimated\n",
      "Random seed set as 9\n",
      "Univatiate modelling\n",
      "inflation factor = 1\n",
      "Rlinear activated\n",
      "Points to be estimated\n",
      "Started training expert 1/2\n",
      "Current learning rate is : 0.0001\n",
      "---------------------------\n",
      "Current (running) training loss at iteration 5000 : 17.756621015167237\n",
      "Current (running) training loss at iteration 10000 : 15.195751393890381\n",
      "Current (running) training loss at iteration 15000 : 12.258879483588537\n",
      "Current (running) training loss at iteration 20000 : 10.411477906143665\n",
      "Current (running) training loss at iteration 25000 : 9.246254623684884\n",
      "Current (running) training loss at iteration 30000 : 8.384997547312578\n",
      "Current (running) training loss at iteration 35000 : 7.6538607461742\n",
      "Current (running) training loss at iteration 40000 : 6.939940826584399\n",
      "Current (running) training loss at iteration 45000 : 6.347580178728369\n",
      "\n",
      "Epoch 0: \n",
      "\n",
      "Train metrics: -------\n",
      "Running (training) loss is 6.166769241419423.\n",
      "Training MAE is 0.6109018921852112.\n",
      "Training MSE is 6.217320919036865.\n",
      "\n",
      "Test metrics: -------\n",
      "Running (test) loss is 2.531465843721078.\n",
      "Test MAE is 0.41959044337272644.\n",
      "Test MSE is 2.5999176502227783.\n",
      "---------------------------\n",
      "Started training expert 2/2\n",
      "Current learning rate is : 0.0001\n",
      "---------------------------\n",
      "Current (running) training loss at iteration 5000 : 17.96179715042114\n",
      "Current (running) training loss at iteration 10000 : 15.206020065498352\n",
      "Current (running) training loss at iteration 15000 : 12.265034304523468\n",
      "Current (running) training loss at iteration 20000 : 10.364678904485702\n",
      "Current (running) training loss at iteration 25000 : 9.214215201129914\n",
      "Current (running) training loss at iteration 30000 : 8.327239037676652\n",
      "Current (running) training loss at iteration 35000 : 7.576155324152538\n",
      "Current (running) training loss at iteration 40000 : 6.867427698718012\n",
      "Current (running) training loss at iteration 45000 : 6.280217243835661\n",
      "\n",
      "Epoch 0: \n",
      "\n",
      "Train metrics: -------\n",
      "Running (training) loss is 6.104157697314459.\n",
      "Training MAE is 0.6065437197685242.\n",
      "Training MSE is 6.1595354080200195.\n",
      "\n",
      "Test metrics: -------\n",
      "Running (test) loss is 2.100450590216405.\n",
      "Test MAE is 0.3844183683395386.\n",
      "Test MSE is 2.1654741764068604.\n",
      "---------------------------\n",
      "Started training Mixture of Experts\n",
      "Current learning rate is : 0.0001\n",
      "---------------------------\n",
      "weights\n",
      "tensor([[[1., 1.]],\n",
      "\n",
      "        [[1., 1.]],\n",
      "\n",
      "        [[1., 1.]],\n",
      "\n",
      "        [[1., 1.]],\n",
      "\n",
      "        [[1., 1.]],\n",
      "\n",
      "        [[1., 1.]],\n",
      "\n",
      "        [[1., 1.]],\n",
      "\n",
      "        [[1., 1.]]], grad_fn=<SoftmaxBackward0>)\n",
      "----------------------------\n",
      "outputs\n",
      "tensor([[[32.1953, 28.9835],\n",
      "         [33.1514, 28.2766],\n",
      "         [33.3362, 33.3020],\n",
      "         [32.7209, 30.4659],\n",
      "         [32.2982, 31.4183],\n",
      "         [33.0633, 28.5640],\n",
      "         [33.9358, 28.9205],\n",
      "         [32.9023, 28.4329],\n",
      "         [33.5337, 32.8103],\n",
      "         [34.6533, 31.2062],\n",
      "         [33.0571, 26.6120],\n",
      "         [32.7822, 31.6588],\n",
      "         [32.9607, 25.5791],\n",
      "         [31.0782, 30.6856],\n",
      "         [28.5917, 29.7453],\n",
      "         [37.1500, 28.6742],\n",
      "         [28.6712, 29.9675],\n",
      "         [28.0657, 27.0364],\n",
      "         [29.4988, 31.8360],\n",
      "         [32.1480, 28.2108],\n",
      "         [32.3677, 26.6831],\n",
      "         [31.2005, 26.6859],\n",
      "         [29.6987, 26.8296],\n",
      "         [26.2265, 29.2062]],\n",
      "\n",
      "        [[27.9523, 31.3607],\n",
      "         [28.9194, 30.5782],\n",
      "         [27.6687, 30.1595],\n",
      "         [29.0235, 29.9342],\n",
      "         [31.5829, 31.1274],\n",
      "         [32.5344, 27.4393],\n",
      "         [29.9061, 28.9632],\n",
      "         [31.7147, 29.9530],\n",
      "         [31.0922, 32.7468],\n",
      "         [30.3994, 29.0265],\n",
      "         [31.8605, 26.8806],\n",
      "         [32.8464, 30.7430],\n",
      "         [30.5586, 29.1607],\n",
      "         [29.5479, 25.9031],\n",
      "         [29.0307, 32.0724],\n",
      "         [33.4078, 27.4913],\n",
      "         [28.7649, 30.0715],\n",
      "         [27.1896, 31.0197],\n",
      "         [30.8940, 26.4250],\n",
      "         [27.9484, 28.7800],\n",
      "         [26.1970, 28.8705],\n",
      "         [29.3078, 28.9353],\n",
      "         [27.9343, 29.7544],\n",
      "         [23.8510, 24.2056]],\n",
      "\n",
      "        [[31.0452, 30.3970],\n",
      "         [29.6163, 32.2589],\n",
      "         [31.7320, 32.3178],\n",
      "         [27.9209, 29.5264],\n",
      "         [32.0662, 34.0248],\n",
      "         [31.8233, 30.6264],\n",
      "         [31.7888, 33.7651],\n",
      "         [34.2204, 33.6578],\n",
      "         [30.9787, 30.5367],\n",
      "         [28.2718, 31.2490],\n",
      "         [32.5637, 31.4389],\n",
      "         [31.2929, 32.9286],\n",
      "         [32.1717, 34.9399],\n",
      "         [30.0273, 32.1229],\n",
      "         [27.6941, 35.5985],\n",
      "         [29.3095, 30.2372],\n",
      "         [28.5595, 34.2589],\n",
      "         [27.8069, 27.9118],\n",
      "         [28.0997, 29.6839],\n",
      "         [29.3354, 26.0833],\n",
      "         [27.8949, 31.8416],\n",
      "         [27.2558, 29.6409],\n",
      "         [27.3552, 31.8634],\n",
      "         [23.1943, 26.9931]],\n",
      "\n",
      "        [[28.0448, 29.4480],\n",
      "         [29.8159, 30.2815],\n",
      "         [28.4200, 31.2940],\n",
      "         [26.0865, 30.2691],\n",
      "         [29.9189, 31.7060],\n",
      "         [33.1522, 32.5752],\n",
      "         [29.8254, 30.8530],\n",
      "         [30.3529, 31.6096],\n",
      "         [29.4890, 30.4423],\n",
      "         [26.4345, 28.8844],\n",
      "         [31.9434, 29.5667],\n",
      "         [30.1820, 30.0213],\n",
      "         [31.2683, 27.9357],\n",
      "         [30.0060, 29.9620],\n",
      "         [28.4105, 31.9503],\n",
      "         [27.9461, 29.8245],\n",
      "         [24.7413, 30.8667],\n",
      "         [27.3665, 27.8912],\n",
      "         [26.0911, 26.4642],\n",
      "         [31.1174, 29.4152],\n",
      "         [27.2248, 29.6438],\n",
      "         [25.0273, 28.5949],\n",
      "         [25.5593, 29.3503],\n",
      "         [22.8021, 26.7091]],\n",
      "\n",
      "        [[27.0170, 30.0735],\n",
      "         [29.0295, 31.8808],\n",
      "         [29.4971, 30.9653],\n",
      "         [28.3047, 29.1712],\n",
      "         [30.8444, 29.4327],\n",
      "         [32.1993, 31.0075],\n",
      "         [30.9147, 29.8267],\n",
      "         [31.8433, 29.5782],\n",
      "         [29.4315, 30.4491],\n",
      "         [28.4450, 29.4746],\n",
      "         [28.8413, 31.4980],\n",
      "         [30.0401, 29.3013],\n",
      "         [29.8165, 26.7573],\n",
      "         [26.6379, 31.0635],\n",
      "         [29.5958, 30.6762],\n",
      "         [29.9179, 26.1055],\n",
      "         [25.6300, 27.4609],\n",
      "         [27.4002, 28.8133],\n",
      "         [28.2604, 26.6643],\n",
      "         [28.7610, 27.3730],\n",
      "         [24.2767, 28.5362],\n",
      "         [23.5889, 27.9934],\n",
      "         [24.8324, 29.2341],\n",
      "         [26.5661, 27.9899]],\n",
      "\n",
      "        [[25.7880, 28.4984],\n",
      "         [31.3455, 29.3023],\n",
      "         [26.4571, 31.1163],\n",
      "         [31.0413, 31.4488],\n",
      "         [28.3362, 28.8593],\n",
      "         [31.7974, 29.8064],\n",
      "         [31.6047, 31.3092],\n",
      "         [32.0248, 27.8421],\n",
      "         [30.0277, 32.1222],\n",
      "         [30.6328, 29.2635],\n",
      "         [29.6083, 26.9128],\n",
      "         [28.6990, 28.8658],\n",
      "         [27.2063, 26.6842],\n",
      "         [30.4911, 30.8553],\n",
      "         [27.2257, 33.3643],\n",
      "         [29.7316, 30.7578],\n",
      "         [24.3430, 28.4958],\n",
      "         [28.6001, 32.7321],\n",
      "         [29.5618, 30.6547],\n",
      "         [26.8271, 33.7698],\n",
      "         [27.0731, 27.9533],\n",
      "         [27.3854, 29.3124],\n",
      "         [26.7056, 25.9357],\n",
      "         [25.7843, 28.9095]],\n",
      "\n",
      "        [[26.4203, 29.7668],\n",
      "         [30.0274, 29.6374],\n",
      "         [27.7261, 29.4689],\n",
      "         [30.9217, 27.6141],\n",
      "         [31.9378, 29.1986],\n",
      "         [31.7404, 30.5034],\n",
      "         [31.6114, 29.4406],\n",
      "         [34.9753, 26.9051],\n",
      "         [29.3311, 29.0160],\n",
      "         [26.2964, 29.9653],\n",
      "         [30.1136, 27.0270],\n",
      "         [31.1876, 29.9083],\n",
      "         [29.7488, 29.1226],\n",
      "         [26.1604, 31.0210],\n",
      "         [25.5829, 32.7565],\n",
      "         [29.7131, 28.9982],\n",
      "         [26.4757, 25.5608],\n",
      "         [31.2095, 31.0086],\n",
      "         [29.0073, 22.9810],\n",
      "         [27.2346, 28.4466],\n",
      "         [29.4094, 28.1627],\n",
      "         [27.1226, 25.3453],\n",
      "         [27.8163, 26.4445],\n",
      "         [26.6099, 25.8085]],\n",
      "\n",
      "        [[28.2578, 27.5637],\n",
      "         [31.9063, 28.8729],\n",
      "         [27.3192, 28.9841],\n",
      "         [25.2766, 29.4132],\n",
      "         [31.4913, 29.4784],\n",
      "         [30.7660, 29.5722],\n",
      "         [29.5266, 29.5469],\n",
      "         [29.0030, 28.4192],\n",
      "         [28.1166, 27.4861],\n",
      "         [29.9617, 28.8914],\n",
      "         [31.8185, 28.4171],\n",
      "         [27.7583, 29.0803],\n",
      "         [30.3486, 27.9923],\n",
      "         [28.9930, 29.4876],\n",
      "         [27.7987, 29.5629],\n",
      "         [28.6655, 29.8353],\n",
      "         [22.9716, 25.0853],\n",
      "         [24.5229, 26.2086],\n",
      "         [26.0130, 26.1337],\n",
      "         [25.5905, 24.3683],\n",
      "         [25.0445, 23.5237],\n",
      "         [25.3516, 24.9679],\n",
      "         [27.7760, 27.6104],\n",
      "         [25.0166, 26.7654]]], grad_fn=<StackBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[8, 1, 1, 2]}, size=[8, 24, 2]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/time_series_experiment/moe_training.ipynb Zelle 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m net \u001b[39m=\u001b[39m train(\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m             epochs \u001b[39m=\u001b[39;49m epochs, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m             n_continous_features\u001b[39m=\u001b[39;49mn_continous_features, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m             n_categorial_features\u001b[39m=\u001b[39;49mn_categorial_features,\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m             p_lag\u001b[39m=\u001b[39;49m  p_lag, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m             future_steps \u001b[39m=\u001b[39;49m future_steps, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m             training_df \u001b[39m=\u001b[39;49m training_df, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             validation_df \u001b[39m=\u001b[39;49m test_df, \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m             feature_columns \u001b[39m=\u001b[39;49m feature_columns,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m             target_column \u001b[39m=\u001b[39;49m target_column, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m             learning_rate\u001b[39m=\u001b[39;49mlearning_rate ,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m             decomp_kernel_size\u001b[39m=\u001b[39;49m decomp_kernel_size, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m             model \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mrlinear\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m             modelling_task \u001b[39m=\u001b[39;49m modelling_task, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m             dataset_name \u001b[39m=\u001b[39;49m dataset_name, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m             moe \u001b[39m=\u001b[39;49m moe, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m             num_of_experts \u001b[39m=\u001b[39;49m number_of_experts\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m             )\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m test_data \u001b[39m=\u001b[39m DataLoader(TimeSeriesDataset(test_df, future_steps\u001b[39m=\u001b[39m future_steps, target_column \u001b[39m=\u001b[39m target_column,feature_columns\u001b[39m=\u001b[39mfeature_columns,p_lag\u001b[39m=\u001b[39mp_lag), batch_size\u001b[39m=\u001b[39mbatch_size,drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borange-rotary-phone-p5q7v5455q7crwj/workspaces/time_series_experiment/moe_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m plot_multistep_forecast(test_data\u001b[39m=\u001b[39mtest_data, dataset_name \u001b[39m=\u001b[39m dataset_name, neural_net\u001b[39m=\u001b[39mnet, future_steps\u001b[39m=\u001b[39mfuture_steps, number_of_forecasts\u001b[39m=\u001b[39mnumber_of_forecasts)\n",
      "File \u001b[0;32m/workspaces/time_series_experiment/utils/training_utils.py:208\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, p_lag, future_steps, n_continous_features, n_categorial_features, training_df, validation_df, feature_columns, dataset_name, target_column, learning_rate, decomp_kernel_size, batch_size, model, moe, num_of_experts, modelling_task, density, depth)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m moe: \n\u001b[1;32m    207\u001b[0m     moe_model \u001b[39m=\u001b[39m MoE(trained_experts)\n\u001b[0;32m--> 208\u001b[0m     moe_model \u001b[39m=\u001b[39m train_expert_or_moe(\n\u001b[1;32m    209\u001b[0m         num_of_experts\u001b[39m=\u001b[39;49mnum_of_experts, \n\u001b[1;32m    210\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate, \n\u001b[1;32m    211\u001b[0m         net\u001b[39m=\u001b[39;49mmoe_model, \n\u001b[1;32m    212\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs, \n\u001b[1;32m    213\u001b[0m         train_data\u001b[39m=\u001b[39;49mtrain_data, \n\u001b[1;32m    214\u001b[0m         density\u001b[39m=\u001b[39;49mdensity, \n\u001b[1;32m    215\u001b[0m         modelling_task\u001b[39m=\u001b[39;49mmodelling_task, \n\u001b[1;32m    216\u001b[0m         n_continous_features\u001b[39m=\u001b[39;49mn_continous_features, \n\u001b[1;32m    217\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[1;32m    218\u001b[0m         val_loss_list\u001b[39m=\u001b[39;49mval_loss_list, \n\u001b[1;32m    219\u001b[0m         val_data\u001b[39m=\u001b[39;49mval_data, \n\u001b[1;32m    220\u001b[0m         train_loss_list\u001b[39m=\u001b[39;49mtrain_loss_list, \n\u001b[1;32m    221\u001b[0m         i\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \n\u001b[1;32m    222\u001b[0m         moe \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    223\u001b[0m     \u001b[39mreturn\u001b[39;00m moe_model\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m: \n",
      "File \u001b[0;32m/workspaces/time_series_experiment/utils/training_utils.py:49\u001b[0m, in \u001b[0;36mtrain_expert_or_moe\u001b[0;34m(net, learning_rate, train_data, modelling_task, n_continous_features, batch_size, val_loss_list, val_data, train_loss_list, i, density, num_of_experts, epochs, moe)\u001b[0m\n\u001b[1;32m     46\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     48\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 49\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m density:\n\u001b[1;32m     51\u001b[0m     loss \u001b[39m=\u001b[39m normal_loss(outputs, labels\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/time_series_experiment/utils/model_utils.py:553\u001b[0m, in \u001b[0;36mMoE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    550\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\n\u001b[1;32m    551\u001b[0m     [expert(x) \u001b[39mfor\u001b[39;00m expert \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperts], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    552\u001b[0m \u001b[39mprint\u001b[39m(outputs)\n\u001b[0;32m--> 553\u001b[0m weights \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mexpand_as(outputs)\n\u001b[1;32m    554\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msum(outputs \u001b[39m*\u001b[39m weights, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[8, 1, 1, 2]}, size=[8, 24, 2]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"
     ]
    }
   ],
   "source": [
    "net = train(\n",
    "            epochs = epochs, \n",
    "            n_continous_features=n_continous_features, \n",
    "            n_categorial_features=n_categorial_features,\n",
    "            p_lag=  p_lag, \n",
    "            future_steps = future_steps, \n",
    "            training_df = training_df, \n",
    "            validation_df = test_df, \n",
    "            feature_columns = feature_columns,\n",
    "            target_column = target_column, \n",
    "            learning_rate=learning_rate ,\n",
    "            decomp_kernel_size= decomp_kernel_size, \n",
    "            batch_size=batch_size, \n",
    "            model = 'rlinear', \n",
    "            modelling_task = modelling_task, \n",
    "            dataset_name = dataset_name, \n",
    "            moe = moe, \n",
    "            num_of_experts = number_of_experts\n",
    "            )\n",
    "test_data = DataLoader(TimeSeriesDataset(test_df, future_steps= future_steps, target_column = target_column,feature_columns=feature_columns,p_lag=p_lag), batch_size=batch_size,drop_last=True)\n",
    "plot_multistep_forecast(test_data=test_data, dataset_name = dataset_name, neural_net=net, future_steps=future_steps, number_of_forecasts=number_of_forecasts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
