{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "ETTm2 = pd.read_csv(\"/workspaces/time_series_experiment/ETT-small/ETTm2.csv\")\n",
    "ETTm1 = pd.read_csv(\"/workspaces/time_series_experiment/ETT-small/ETTm1.csv\")\n",
    "ETTh1 = pd.read_csv(\"/workspaces/time_series_experiment/ETT-small/ETTm1.csv\")\n",
    "ETTh2 = pd.read_csv(\"/workspaces/time_series_experiment/ETT-small/ETTm1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date_column_and_drop_it(df, date_column_name:str, remain_same = True):\n",
    "    df = df.copy()\n",
    "    if remain_same: \n",
    "        df.drop(date_column_name, axis = 1, inplace=True)\n",
    "    else: \n",
    "        df['day'] = df[date_column_name].dt.day\n",
    "        df['month'] = df[date_column_name].dt.month\n",
    "        df['hour'] = df[date_column_name].dt.hour\n",
    "        df['minute'] = df[date_column_name].dt.minute\n",
    "        df['weekday'] = df[date_column_name].dt.dayofweek\n",
    "        df.drop(date_column_name, axis = 1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def split_dataset(df, train_split_month=12, val_split_month=16, test_split_month=20): \n",
    "    data = df.copy()\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    training_df = transform_date_column_and_drop_it(data[data['date'] < data['date'].min() + pd.DateOffset(months=train_split_month)],'date')\n",
    "    val_df = transform_date_column_and_drop_it(data[data['date'] > data['date'].min() + pd.DateOffset(months=train_split_month)][data['date'] < data['date'].min() + pd.DateOffset(months=val_split_month)],'date')\n",
    "    test_df = transform_date_column_and_drop_it(data[data['date'] > data['date'].min() + pd.DateOffset(months=val_split_month)][data['date'] < data['date'].min() + pd.DateOffset(months=test_split_month)],'date')\n",
    "    return training_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self,df, target_column,future_steps, p_lag=0):\n",
    "        self.df = df\n",
    "        self.p_lag = p_lag\n",
    "        self.len_df_minus_lag = len(self.df) - p_lag - future_steps\n",
    "        self.target_column = target_column\n",
    "        self.future_steps = future_steps\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len_df_minus_lag\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_p_lag = torch.tensor(self.df.iloc[(idx):(idx + self.p_lag),:].astype(float).to_numpy().transpose().reshape(1,-1), requires_grad=True)\n",
    "        target = torch.tensor(self.df[self.target_column].iloc[(idx + self.p_lag): (idx + self.p_lag + self.future_steps),:].astype(float).to_numpy()).reshape(1,-1)\n",
    "        return input_p_lag, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class DecompositionLayer(nn.Module):\n",
    "    def __init__(self, kernel_size, n_features):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=1, padding=0) \n",
    "        self.n_features = n_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_of_pads = (self.kernel_size - 1) // 2\n",
    "        if self.kernel_size > self.n_features: \n",
    "            front = x[:, 0:1, :].repeat(1, num_of_pads + 1, 1)\n",
    "        else: \n",
    "            front = x[:, 0:1, :].repeat(1, num_of_pads, 1)\n",
    "        end = x[:, -1:, :].repeat(1, num_of_pads, 1)\n",
    "        x_padded = torch.cat([front, x, end], dim=1)\n",
    "        #print('xpad')\n",
    "        #print(x_padded.shape)\n",
    "        x_trend = self.avg(x_padded.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        #print('xtrend')\n",
    "        #print(x_trend.shape)\n",
    "        x_seasonal = x - x_trend\n",
    "        #print('xseason')\n",
    "        #print(x_seasonal.shape)\n",
    "        return x_seasonal, x_trend\n",
    "\n",
    "class ARNet(nn.Module):\n",
    "    def __init__(self, p_lag, n_features, future_steps, decomp_kernel_size = 7, batch_size = 8):\n",
    "        super(ARNet, self).__init__()\n",
    "        self.input_trend_layer = nn.Linear(p_lag * n_features, math.ceil(p_lag * n_features/1.5))\n",
    "        self.output_trend_layer = nn.Linear(math.ceil(p_lag * n_features/1.5), future_steps)\n",
    "        self.input_seasonal_layer = nn.Linear(p_lag * n_features, math.ceil(p_lag * n_features/1.5))\n",
    "        self.output_seasonal_layer = nn.Linear(math.ceil(p_lag * n_features/1.5), future_steps)\n",
    "        self.decomp_layer = DecompositionLayer(decomp_kernel_size, n_features)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.p_lag = p_lag\n",
    "        self.batch_size = batch_size\n",
    "        self.n_features = n_features\n",
    "        self.future_steps = future_steps\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.float()\n",
    "        #print(input.shape)\n",
    "        input_season, input_trend = self.decomp_layer(input)\n",
    "        #print(input_season.shape)\n",
    "        x_season = self.input_seasonal_layer(input_season.reshape(self.batch_size, self.p_lag*self.n_features))\n",
    "        y_hat_season = self.output_seasonal_layer(x_season)\n",
    "        x_trend = self.input_trend_layer(input_trend.reshape(self.batch_size, self.p_lag*self.n_features))\n",
    "        y_hat_trend = self.output_trend_layer(x_trend)\n",
    "        return y_hat_season + y_hat_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def RSE(pred, true):\n",
    "    return np.sqrt(np.sum((true - pred) ** 2)) / np.sqrt(np.sum((true - true.mean() + 1e-12) ** 2))\n",
    "\n",
    "def CORR(pred, true):\n",
    "    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)\n",
    "    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))\n",
    "    d += 1e-12\n",
    "    return 0.01*(u / d).mean(-1)\n",
    "\n",
    "def MAE(pred, true):\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "def MSE(pred, true):\n",
    "    return np.mean((pred - true) ** 2)\n",
    "\n",
    "def RMSE(pred, true):\n",
    "    return np.sqrt(MSE(pred, true))\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    return np.mean(np.abs((pred - true) / (true + 1e-12)))\n",
    "\n",
    "def MSPE(pred, true):\n",
    "    return np.mean(np.square((pred - true) / (true+ 1e-12)))\n",
    "\n",
    "def metric(pred, true):\n",
    "    mae = MAE(pred, true)\n",
    "    mse = MSE(pred, true)\n",
    "    rmse = RMSE(pred, true)\n",
    "    mape = MAPE(pred, true)\n",
    "    mspe = MSPE(pred, true)\n",
    "    rse = RSE(pred, true)\n",
    "    corr = CORR(pred, true)\n",
    "\n",
    "    return mae, mse, rmse, mape, mspe, rse, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, p_lag, future_steps, n_features, training_df, validation_df, target_column = ['OT'], learning_rate=1.e-4, decomp_kernel_size= 7, batch_size = 8): \n",
    "    net = ARNet(p_lag=p_lag, n_features=n_features, future_steps=future_steps, decomp_kernel_size=decomp_kernel_size, batch_size=batch_size)\n",
    "\n",
    "    train_data = DataLoader(TimeSeriesDataset(training_df, future_steps= future_steps, target_column = target_column,p_lag=p_lag), batch_size=batch_size, drop_last=True)\n",
    "    train_loss_list = []\n",
    "    val_data = DataLoader(TimeSeriesDataset(validation_df,future_steps= future_steps, target_column = target_column,p_lag=p_lag), batch_size=batch_size, drop_last=True)\n",
    "    val_loss_list = []\n",
    "\n",
    "    torch.set_grad_enabled(True)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "\n",
    "        train_counter = 0\n",
    "        val_counter = 0\n",
    "\n",
    "        running_train_loss = 0.\n",
    "        running_val_loss = 0.\n",
    "        running_train_mae  = 0.\n",
    "        running_train_mse  = 0.\n",
    "        running_train_rmse = 0.\n",
    "        running_train_mape = 0.\n",
    "        running_train_mspe = 0.\n",
    "        running_train_rse  = 0.\n",
    "        running_train_corr = 0.\n",
    "        \n",
    "        running_val_mae  = 0.\n",
    "        running_val_mse  = 0.\n",
    "        running_val_rmse = 0.\n",
    "        running_val_mape = 0.\n",
    "        running_val_mspe = 0.\n",
    "        running_val_rse  = 0.\n",
    "        running_val_corr = 0.\n",
    "\n",
    "        if epoch + 1 != 1: \n",
    "            learning_rate = learning_rate / 2\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        print(f\"Current learning rate is : {learning_rate}\")  \n",
    "        print(\"---------------------------\")\n",
    "        for i, data in enumerate(train_data):\n",
    "            inputs, labels = data\n",
    "            labels = labels.squeeze(0).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = net.criterion(outputs, labels.squeeze(1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs_array = outputs.detach().cpu().numpy()\n",
    "            labels_array = labels.squeeze(2).detach().cpu().numpy()\n",
    "            mae, mse, rmse, mape, mspe, rse, corr = metric(pred=outputs_array, true=labels_array)\n",
    "            running_train_mae  += mae\n",
    "            running_train_mse  += mse\n",
    "            running_train_rmse += rmse\n",
    "            running_train_mape += mape\n",
    "            running_train_mspe += mspe\n",
    "            running_train_rse  += rse\n",
    "            running_train_corr += corr\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            train_counter += batch_size\n",
    "            if train_counter % 5000 == 0: \n",
    "                print(f\"Current (running) training loss at iteration {train_counter} : {running_train_loss/train_counter}\")\n",
    "\n",
    "        train_loss_list.append(running_train_loss/train_counter)\n",
    "            \n",
    "        for i, data in enumerate(val_data):\n",
    "            inputs, test_labels = data\n",
    "            test_labels = test_labels.squeeze(0).float()\n",
    "            output = net(inputs)\n",
    "            val_loss = net.criterion(output, test_labels.squeeze(1))\n",
    "            running_val_loss += val_loss.item()\n",
    "\n",
    "            output_array = output.detach().cpu().numpy()\n",
    "            test_labels_array = test_labels.squeeze(2).detach().cpu().numpy()\n",
    "            mae, mse, rmse, mape, mspe, rse, corr = metric(pred=output_array, true=test_labels_array)\n",
    "            running_val_mae  += mae\n",
    "            running_val_mse  += mse\n",
    "            running_val_rmse += rmse\n",
    "            running_val_mape += mape\n",
    "            running_val_mspe += mspe\n",
    "            running_val_rse  += rse\n",
    "            running_val_corr += corr\n",
    "\n",
    "            val_counter += batch_size\n",
    "        val_loss_list.append(running_val_loss/val_counter)\n",
    "\n",
    "\n",
    "        if epoch % 1 == 0: \n",
    "            print(f\"Epoch {epoch}: \")\n",
    "            print(\"\")\n",
    "            print(\"Train metrics: -------\")\n",
    "            print(f\"Running (training) loss is {running_train_loss/train_counter}.\")\n",
    "            print(f\"Training MAE is {running_train_mae/train_counter}.\")\n",
    "            print(f\"Training MSE is {running_train_mse/train_counter}.\")\n",
    "            print(f\"Training RMSE is {running_train_rmse/train_counter}.\")\n",
    "            print(f\"Training MAPE is {running_train_mape/train_counter}.\")\n",
    "            print(f\"Training MSPE is {running_train_mspe/train_counter}.\")\n",
    "            print(f\"Training RSE is {running_train_rse/train_counter}.\")\n",
    "            print(f\"Training CORR is {running_train_corr/train_counter}.\")\n",
    "            print(\"\")\n",
    "            print(\"Val metrics: -------\")\n",
    "            print(f\"Running (validation) loss is {running_val_loss/val_counter}.\")\n",
    "            print(f\"Validation MAE is {running_val_mae/train_counter}.\")\n",
    "            print(f\"Validation MSE is {running_val_mse/train_counter}.\")\n",
    "            print(f\"Validation RMSE is {running_val_rmse/train_counter}.\")\n",
    "            print(f\"Validation MAPE is {running_val_mape/train_counter}.\")\n",
    "            print(f\"Validation MSPE is {running_val_mspe/train_counter}.\")\n",
    "            print(f\"Validation RSE is {running_val_rse/train_counter}.\")\n",
    "            print(f\"Validation CORR is {running_val_corr/train_counter}.\")\n",
    "            print(\"---------------------------\")\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_multistep_forecast(test_data, neural_net, future_steps, number_of_forecasts= 100): \n",
    "    output_list = []\n",
    "    target_list = []\n",
    "    for i, data in enumerate(test_data):\n",
    "        inputs, labels = data\n",
    "        output = neural_net(inputs)\n",
    "        if i > number_of_forecasts: \n",
    "            break\n",
    "        [output_list.append(out) for out in output.tolist()]\n",
    "        [target_list.append(tar) for tar in labels.squeeze(1,2).tolist()]\n",
    "\n",
    "    target = []\n",
    "    for i in range(len(target_list)): \n",
    "        if i == 0: \n",
    "            target = target_list[i]\n",
    "        else: \n",
    "            target.append(target_list[i][len(target_list[i])-1])\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    plt.plot(range(0, len(target)), target, 'g', label='target time series')\n",
    "    for i, output in enumerate(output_list, start=0): \n",
    "        if i == 0:\n",
    "            plt.plot(range(i, i +future_steps), output, color='#F39C12',linewidth=1, linestyle='-.',alpha=0.1, label='pred time series' + \"\\n\" + f'{future_steps} each')\n",
    "        else: \n",
    "            plt.plot(range(i, i +future_steps), output, color='#F39C12',linewidth=1, linestyle='-.',alpha=0.1)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Oil Temparature (Target variable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global parameters \n",
    "p_lag = 96\n",
    "future_steps = 24\n",
    "batch_size = 8\n",
    "epochs = 8\n",
    "learning_rate=1.e-4\n",
    "decomp_kernel_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2117/1238007648.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  val_df = transform_date_column_and_drop_it(data[data['date'] > data['date'].min() + pd.DateOffset(months=train_split_month)][data['date'] < data['date'].min() + pd.DateOffset(months=val_split_month)],'date')\n",
      "/tmp/ipykernel_2117/1238007648.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  test_df = transform_date_column_and_drop_it(data[data['date'] > data['date'].min() + pd.DateOffset(months=val_split_month)][data['date'] < data['date'].min() + pd.DateOffset(months=test_split_month)],'date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate is : 0.0001\n",
      "---------------------------\n",
      "Current (running) training loss at iteration 5000 : 4.5071632520198825\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/time_series_experiment/data/inspection.ipynb Zelle 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m training_df, val_df, test_df \u001b[39m=\u001b[39m split_dataset(ETTm2)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m net \u001b[39m=\u001b[39m train(\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m             epochs \u001b[39m=\u001b[39;49m epochs, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m             n_features\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(training_df\u001b[39m.\u001b[39;49mcolumns), \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m             p_lag\u001b[39m=\u001b[39;49m  p_lag, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m             future_steps \u001b[39m=\u001b[39;49m future_steps, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m             training_df \u001b[39m=\u001b[39;49m training_df, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m             validation_df \u001b[39m=\u001b[39;49m val_df, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m             target_column \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mOT\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m             learning_rate\u001b[39m=\u001b[39;49mlearning_rate ,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m             decomp_kernel_size\u001b[39m=\u001b[39;49m decomp_kernel_size, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m             )\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m test_data \u001b[39m=\u001b[39m DataLoader(TimeSeriesDataset(val_df,future_steps\u001b[39m=\u001b[39m future_steps, target_column \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mOT\u001b[39m\u001b[39m'\u001b[39m],p_lag\u001b[39m=\u001b[39mp_lag), batch_size\u001b[39m=\u001b[39mbatch_size,drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m plot_multistep_forecast(test_data\u001b[39m=\u001b[39mtest_data, neural_net\u001b[39m=\u001b[39mnet, future_steps\u001b[39m=\u001b[39mfuture_steps)\n",
      "\u001b[1;32m/workspaces/time_series_experiment/data/inspection.ipynb Zelle 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(net\u001b[39m.\u001b[39mparameters(), \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m outputs_array \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m labels_array \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    392\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     adam(\n\u001b[1;32m    169\u001b[0m         params_with_grad,\n\u001b[1;32m    170\u001b[0m         grads,\n\u001b[1;32m    171\u001b[0m         exp_avgs,\n\u001b[1;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    174\u001b[0m         state_steps,\n\u001b[1;32m    175\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m         has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[1;32m    177\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    178\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    179\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    180\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    185\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    186\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    187\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    188\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m func(params,\n\u001b[1;32m    319\u001b[0m      grads,\n\u001b[1;32m    320\u001b[0m      exp_avgs,\n\u001b[1;32m    321\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    322\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    323\u001b[0m      state_steps,\n\u001b[1;32m    324\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    325\u001b[0m      has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[1;32m    326\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    327\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    328\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    329\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    330\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    331\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    332\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    333\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    334\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    335\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:373\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    370\u001b[0m step_t \u001b[39m=\u001b[39m state_steps[i]\n\u001b[1;32m    372\u001b[0m \u001b[39m# If compiling, the compiler will handle cudagraph checks, see note [torch.compile x capturable]\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39;49m_utils\u001b[39m.\u001b[39;49mis_compiling() \u001b[39mand\u001b[39;00m capturable:\n\u001b[1;32m    374\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m         (param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda) \u001b[39mor\u001b[39;00m (param\u001b[39m.\u001b[39mis_xla \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_xla)\n\u001b[1;32m    376\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA or XLA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m \u001b[39m# update step\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:857\u001b[0m, in \u001b[0;36mis_compiling\u001b[0;34m()\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_compiling\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    852\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[39m    Indicates whether we are tracing/compiling with torch.compile() or torch.export().\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \n\u001b[1;32m    855\u001b[0m \u001b[39m    TODO(khabinov): we should deprecate this function and use torch.compiler.is_compiling().\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcompiler\u001b[39m.\u001b[39;49mis_compiling()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ETTm2 \n",
    "\n",
    "#different datasets\n",
    "training_df, val_df, test_df = split_dataset(ETTm2)\n",
    "\n",
    "#training\n",
    "net = train(\n",
    "            epochs = epochs, \n",
    "            n_features=len(training_df.columns), \n",
    "            p_lag=  p_lag, \n",
    "            future_steps = future_steps, \n",
    "            training_df = training_df, \n",
    "            validation_df = val_df, \n",
    "            target_column = ['OT'], \n",
    "            learning_rate=learning_rate ,\n",
    "            decomp_kernel_size= decomp_kernel_size, \n",
    "            batch_size=batch_size\n",
    "            )\n",
    "\n",
    "test_data = DataLoader(TimeSeriesDataset(val_df,future_steps= future_steps, target_column = ['OT'],p_lag=p_lag), batch_size=batch_size,drop_last=True)\n",
    "plot_multistep_forecast(test_data=test_data, neural_net=net, future_steps=future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate is : 0.0001\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2117/1238007648.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  val_df = transform_date_column_and_drop_it(data[data['date'] > data['date'].min() + pd.DateOffset(months=train_split_month)][data['date'] < data['date'].min() + pd.DateOffset(months=val_split_month)],'date')\n",
      "/tmp/ipykernel_2117/1238007648.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  test_df = transform_date_column_and_drop_it(data[data['date'] > data['date'].min() + pd.DateOffset(months=val_split_month)][data['date'] < data['date'].min() + pd.DateOffset(months=test_split_month)],'date')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, 672]' is invalid for input of size 10752",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/time_series_experiment/data/inspection.ipynb Zelle 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m training_df, val_df, test_df \u001b[39m=\u001b[39m split_dataset(ETTm1)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m net \u001b[39m=\u001b[39m train(\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m             epochs \u001b[39m=\u001b[39;49m epochs, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m             n_features\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(training_df\u001b[39m.\u001b[39;49mcolumns), \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m             p_lag\u001b[39m=\u001b[39;49m  p_lag, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m             future_steps \u001b[39m=\u001b[39;49m future_steps, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m             training_df \u001b[39m=\u001b[39;49m training_df, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m             validation_df \u001b[39m=\u001b[39;49m val_df, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m             target_column \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mOT\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m             learning_rate\u001b[39m=\u001b[39;49mlearning_rate ,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m             decomp_kernel_size\u001b[39m=\u001b[39;49m decomp_kernel_size, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m             )\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m test_data \u001b[39m=\u001b[39m DataLoader(TimeSeriesDataset(val_df,future_steps\u001b[39m=\u001b[39m future_steps, target_column \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mOT\u001b[39m\u001b[39m'\u001b[39m],p_lag\u001b[39m=\u001b[39mp_lag), batch_size\u001b[39m=\u001b[39mbatch_size,drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m plot_multistep_forecast(test_data\u001b[39m=\u001b[39mtest_data, neural_net\u001b[39m=\u001b[39mnet, future_steps\u001b[39m=\u001b[39mfuture_steps)\n",
      "\u001b[1;32m/workspaces/time_series_experiment/data/inspection.ipynb Zelle 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m loss \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mcriterion(outputs, labels\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/workspaces/time_series_experiment/data/inspection.ipynb Zelle 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m input_season, input_trend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecomp_layer(\u001b[39minput\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m#print(input_season.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m x_season \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_seasonal_layer(input_season\u001b[39m.\u001b[39;49mreshape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_lag\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_features))\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m y_hat_season \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_seasonal_layer(x_season)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m x_trend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_trend_layer(input_trend\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_lag\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[8, 672]' is invalid for input of size 10752"
     ]
    }
   ],
   "source": [
    "#ETTm1 \n",
    "\n",
    "#different datasets\n",
    "training_df, val_df, test_df = split_dataset(ETTm1)\n",
    "\n",
    "#training\n",
    "net = train(\n",
    "            epochs = epochs, \n",
    "            n_features=len(training_df.columns), \n",
    "            p_lag=  p_lag, \n",
    "            future_steps = future_steps, \n",
    "            training_df = training_df, \n",
    "            validation_df = val_df, \n",
    "            target_column = ['OT'], \n",
    "            learning_rate=learning_rate ,\n",
    "            decomp_kernel_size= decomp_kernel_size, \n",
    "            batch_size=batch_size\n",
    "            )\n",
    "\n",
    "test_data = DataLoader(TimeSeriesDataset(val_df,future_steps= future_steps, target_column = ['OT'],p_lag=p_lag), batch_size=batch_size,drop_last=True)\n",
    "plot_multistep_forecast(test_data=test_data, neural_net=net, future_steps=future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate is : 0.0001\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2117/1238007648.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  val_df = transform_date_column_and_drop_it(data[data['date'] > data['date'].min() + pd.DateOffset(months=train_split_month)][data['date'] < data['date'].min() + pd.DateOffset(months=val_split_month)],'date')\n",
      "/tmp/ipykernel_2117/1238007648.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  test_df = transform_date_column_and_drop_it(data[data['date'] > data['date'].min() + pd.DateOffset(months=val_split_month)][data['date'] < data['date'].min() + pd.DateOffset(months=test_split_month)],'date')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, 672]' is invalid for input of size 10752",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/time_series_experiment/data/inspection.ipynb Zelle 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m training_df, val_df, test_df \u001b[39m=\u001b[39m split_dataset(ETTh1)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m net \u001b[39m=\u001b[39m train(\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m             epochs \u001b[39m=\u001b[39;49m epochs, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m             n_features\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(training_df\u001b[39m.\u001b[39;49mcolumns), \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m             p_lag\u001b[39m=\u001b[39;49m  p_lag, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m             future_steps \u001b[39m=\u001b[39;49m future_steps, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m             training_df \u001b[39m=\u001b[39;49m training_df, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m             validation_df \u001b[39m=\u001b[39;49m val_df, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m             target_column \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mOT\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m             learning_rate\u001b[39m=\u001b[39;49mlearning_rate ,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m             decomp_kernel_size\u001b[39m=\u001b[39;49m decomp_kernel_size, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m             )\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m test_data \u001b[39m=\u001b[39m DataLoader(TimeSeriesDataset(val_df,future_steps\u001b[39m=\u001b[39m future_steps, target_column \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mOT\u001b[39m\u001b[39m'\u001b[39m],p_lag\u001b[39m=\u001b[39mp_lag), batch_size\u001b[39m=\u001b[39mbatch_size,drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m plot_multistep_forecast(test_data\u001b[39m=\u001b[39mtest_data, neural_net\u001b[39m=\u001b[39mnet, future_steps\u001b[39m=\u001b[39mfuture_steps)\n",
      "\u001b[1;32m/workspaces/time_series_experiment/data/inspection.ipynb Zelle 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m loss \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mcriterion(outputs, labels\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/workspaces/time_series_experiment/data/inspection.ipynb Zelle 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m input_season, input_trend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecomp_layer(\u001b[39minput\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m x_season \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_seasonal_layer(input_season\u001b[39m.\u001b[39;49mreshape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_lag\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_features))\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m y_hat_season \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_seasonal_layer(x_season)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-cod-jx9v4gg95j3pg9r/workspaces/time_series_experiment/data/inspection.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m x_trend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_trend_layer(input_trend\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_lag\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[8, 672]' is invalid for input of size 10752"
     ]
    }
   ],
   "source": [
    "#ETTh1 \n",
    "\n",
    "#different datasets\n",
    "training_df, val_df, test_df = split_dataset(ETTh1)\n",
    "\n",
    "#training\n",
    "net = train(\n",
    "            epochs = epochs, \n",
    "            n_features=len(training_df.columns), \n",
    "            p_lag=  p_lag, \n",
    "            future_steps = future_steps, \n",
    "            training_df = training_df, \n",
    "            validation_df = val_df, \n",
    "            target_column = ['OT'], \n",
    "            learning_rate=learning_rate ,\n",
    "            decomp_kernel_size= decomp_kernel_size, \n",
    "            batch_size=batch_size\n",
    "            )\n",
    "\n",
    "test_data = DataLoader(TimeSeriesDataset(val_df,future_steps= future_steps, target_column = ['OT'],p_lag=p_lag), batch_size=batch_size,drop_last=True)\n",
    "plot_multistep_forecast(test_data=test_data, neural_net=net, future_steps=future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2117/1238007648.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  val_df = transform_date_column_and_drop_it(data[data['date'] > data['date'].min() + pd.DateOffset(months=train_split_month)][data['date'] < data['date'].min() + pd.DateOffset(months=val_split_month)],'date')\n",
      "/tmp/ipykernel_2117/1238007648.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  test_df = transform_date_column_and_drop_it(data[data['date'] > data['date'].min() + pd.DateOffset(months=val_split_month)][data['date'] < data['date'].min() + pd.DateOffset(months=test_split_month)],'date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate is : 0.0001\n",
      "---------------------------\n",
      "Current (running) training loss at iteration 5000 : 2.5049675294160845\n",
      "Current (running) training loss at iteration 10000 : 1.8899359775081277\n",
      "Current (running) training loss at iteration 15000 : 1.754740403136611\n",
      "Current (running) training loss at iteration 20000 : 1.5515935640998184\n",
      "Current (running) training loss at iteration 25000 : 1.476702705897689\n",
      "Current (running) training loss at iteration 30000 : 1.3784862323428193\n"
     ]
    }
   ],
   "source": [
    "#ETTh2\n",
    "\n",
    "#different datasets\n",
    "training_df, val_df, test_df = split_dataset(ETTh2)\n",
    "\n",
    "#training\n",
    "net = train(\n",
    "            epochs = epochs, \n",
    "            n_features=len(training_df.columns), \n",
    "            p_lag=  p_lag, \n",
    "            future_steps = future_steps, \n",
    "            training_df = training_df, \n",
    "            validation_df = val_df, \n",
    "            target_column = ['OT'], \n",
    "            learning_rate=learning_rate ,\n",
    "            decomp_kernel_size= decomp_kernel_size, \n",
    "            batch_size=batch_size\n",
    "            )\n",
    "\n",
    "test_data = DataLoader(TimeSeriesDataset(val_df,future_steps= future_steps, target_column = ['OT'],p_lag=p_lag), batch_size=batch_size,drop_last=True)\n",
    "plot_multistep_forecast(test_data=test_data, neural_net=net, future_steps=future_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
