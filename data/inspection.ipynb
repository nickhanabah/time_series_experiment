{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self,path, p_lag=0, d_lag=0, test=False):\n",
    "        self.df = pd.read_csv(path)\n",
    "        if test: \n",
    "            self.df = self.df.iloc[round(len(self.df)*0.8):len(self.df),:]\n",
    "        else: \n",
    "            self.df = self.df.iloc[0:round(len(self.df)*0.8),:]\n",
    "        self.p_lag = p_lag\n",
    "        self.d_lag = d_lag\n",
    "        self.len_df_minus_lag = len(self.df) - p_lag - d_lag\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len_df_minus_lag\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_p_lag = torch.tensor(self.df.iloc[(idx+self.d_lag):(idx+self.d_lag + self.p_lag),1].astype(float).values, requires_grad=True)\n",
    "        if self.d_lag >0: \n",
    "            input_d_lag = torch.tensor(self.df.iloc[idx:(idx + self.p_lag),1].astype(float).values)\n",
    "            input = input_p_lag - input_d_lag\n",
    "        else: \n",
    "            input = input_p_lag\n",
    "        target = torch.tensor(self.df.iloc[(idx + self.p_lag + 1),1].astype(float))\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class ARNet(nn.Module):\n",
    "    def __init__(self, p_lag):\n",
    "        super(ARNet, self).__init__()\n",
    "        self.input_layer = nn.Linear(p_lag, math.ceil(p_lag/2))\n",
    "        self.output_layer = nn.Linear(math.ceil(p_lag/2), 1)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.float()\n",
    "        y_hat = self.input_layer(input)\n",
    "        return torch.exp(self.output_layer(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \n",
      "Running (training) loss is 86.76841869484.\n",
      "Test loss is 101.14319485751065.\n",
      "---------------------------\n",
      "Epoch 25: \n",
      "Running (training) loss is 29.541790803628167.\n",
      "Test loss is 33.58371230155972.\n",
      "---------------------------\n",
      "Epoch 50: \n",
      "Running (training) loss is 17.47164522427308.\n",
      "Test loss is 19.51238501907044.\n",
      "---------------------------\n",
      "Epoch 75: \n",
      "Running (training) loss is 13.315962069774061.\n",
      "Test loss is 14.993977755547732.\n",
      "---------------------------\n",
      "Epoch 100: \n",
      "Running (training) loss is 11.173915895246255.\n",
      "Test loss is 12.709034598933568.\n",
      "---------------------------\n",
      "Epoch 125: \n",
      "Running (training) loss is 9.881372679035126.\n",
      "Test loss is 11.254651317274107.\n",
      "---------------------------\n",
      "Epoch 150: \n",
      "Running (training) loss is 8.979158772050058.\n",
      "Test loss is 10.480201761419973.\n",
      "---------------------------\n",
      "Epoch 175: \n",
      "Running (training) loss is 8.360660576344637.\n",
      "Test loss is 9.91770275984859.\n",
      "---------------------------\n",
      "Epoch 200: \n",
      "Running (training) loss is 7.8865543408406324.\n",
      "Test loss is 9.668385164532339.\n",
      "---------------------------\n",
      "Epoch 225: \n",
      "Running (training) loss is 7.531262553140181.\n",
      "Test loss is 9.340417517974991.\n",
      "---------------------------\n",
      "Epoch 250: \n",
      "Running (training) loss is 7.233720839944275.\n",
      "Test loss is 8.989924279933748.\n",
      "---------------------------\n",
      "Epoch 275: \n",
      "Running (training) loss is 6.990601384825035.\n",
      "Test loss is 8.749236403308192.\n",
      "---------------------------\n",
      "Epoch 300: \n",
      "Running (training) loss is 6.781009333315957.\n",
      "Test loss is 8.558640469304967.\n",
      "---------------------------\n",
      "Epoch 325: \n",
      "Running (training) loss is 6.606900615118251.\n",
      "Test loss is 8.391920436088824.\n",
      "---------------------------\n",
      "Epoch 350: \n",
      "Running (training) loss is 6.451576537171803.\n",
      "Test loss is 8.280365686560017.\n",
      "---------------------------\n",
      "Epoch 375: \n",
      "Running (training) loss is 6.309502653133842.\n",
      "Test loss is 8.198515529862709.\n",
      "---------------------------\n",
      "Epoch 400: \n",
      "Running (training) loss is 6.186845723636295.\n",
      "Test loss is 8.115991867653797.\n",
      "---------------------------\n",
      "Epoch 425: \n",
      "Running (training) loss is 6.080837875780599.\n",
      "Test loss is 7.983968677118638.\n",
      "---------------------------\n",
      "Epoch 450: \n",
      "Running (training) loss is 5.979836598996721.\n",
      "Test loss is 7.827919577729944.\n",
      "---------------------------\n",
      "Epoch 475: \n",
      "Running (training) loss is 5.891390952034588.\n",
      "Test loss is 7.745880847004132.\n",
      "---------------------------\n",
      "Epoch 500: \n",
      "Running (training) loss is 5.810365837448691.\n",
      "Test loss is 7.637556636868817.\n",
      "---------------------------\n",
      "Epoch 525: \n",
      "Running (training) loss is 5.739288610808077.\n",
      "Test loss is 7.520555500446166.\n",
      "---------------------------\n",
      "Epoch 550: \n",
      "Running (training) loss is 5.671613940862181.\n",
      "Test loss is 7.427783399789279.\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "p_lag = 24\n",
    "\n",
    "net = ARNet(p_lag)\n",
    "\n",
    "train_data = TimeSeriesDataset(path='Electric_Production.csv',p_lag=p_lag,d_lag=0,test=False)\n",
    "train_loss_list = []\n",
    "test_data = TimeSeriesDataset(path='Electric_Production.csv',p_lag=p_lag,d_lag=0,test=True)\n",
    "test_loss_list = []\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.000001, momentum=0.9)\n",
    "running_train_loss = 0.\n",
    "running_test_loss = 0.\n",
    "train_counter = 1\n",
    "test_counter = 1\n",
    "for epoch in range(2500): \n",
    "    for i, data in enumerate(train_data):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(torch.log(inputs))\n",
    "        loss = torch.sqrt(net.criterion(outputs, labels.unsqueeze(0).float()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "        train_counter += 1\n",
    "        train_loss_list.append(running_train_loss/train_counter)\n",
    "\n",
    "    for i, data in enumerate(test_data):\n",
    "        inputs, test_labels = data\n",
    "        output = net(torch.log(inputs))\n",
    "        test_loss = torch.sqrt(net.criterion(output, test_labels.unsqueeze(0).float()))\n",
    "        running_test_loss += test_loss.item()\n",
    "        test_counter += 1\n",
    "        test_loss_list.append(running_test_loss/test_counter)\n",
    "\n",
    "    if epoch % 25 == 0: \n",
    "        print(f\"Epoch {epoch}: \")\n",
    "        print(f\"Running (training) loss is {running_train_loss/train_counter}.\")\n",
    "        print(f\"Test loss is {running_test_loss/test_counter}.\")\n",
    "        print(\"---------------------------\")\n",
    "\n",
    "for i, data in enumerate(test_data):\n",
    "    inputs, labels = data\n",
    "    output = net(torch.log(inputs))\n",
    "    print(f\"predicted output is {output.item()} and true value is {labels.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
